{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For code explanation: Review \"Training a Binary Classifier\" section in chapter 3: Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for machine learning classification\n",
    "import numpy as np\n",
    "from numpy.core.numeric import cross\n",
    "import pandas as pd\n",
    "from scipy.sparse.construct import rand\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>left</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.38             0.53               2                   157   \n",
       "1                0.80             0.86               5                   262   \n",
       "2                0.11             0.88               7                   272   \n",
       "3                0.72             0.87               5                   223   \n",
       "4                0.37             0.52               2                   159   \n",
       "\n",
       "   time_spend_company  Work_accident  promotion_last_5years  left  \n",
       "0                   3              0                      0     1  \n",
       "1                   6              0                      0     1  \n",
       "2                   4              0                      0     1  \n",
       "3                   5              0                      0     1  \n",
       "4                   3              0                      0     1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the employee retention dataset\n",
    "df = pd.read_csv('employee_retention.csv')\n",
    "\n",
    "# Uncomment below to see statistical description of the data\n",
    "#print(df.describe())\n",
    "\n",
    "# Uncomment below to see the first few rows of the data\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation: Separate features from target variable\n",
    "# X contains the first 7 columns (features/predictors)\n",
    "# y contains the 8th column (target variable: whether employee left or not)\n",
    "\n",
    "X = df.iloc[:, 0:7]  # Features: satisfaction_level through promotion_last_5years\n",
    "y = df.iloc[:, 7]     # Target: 'left' (1 = employee left, 0 = employee stayed)\n",
    "\n",
    "# Uncomment to inspect the features and target\n",
    "#print(X)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "# Using 20% of data for testing (80% for training)\n",
    "# random_state=42 ensures reproducibility of results\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train the Stochastic Gradient Descent Classifier\n",
    "# SGDClassifier is efficient for large datasets and uses iterative learning\n",
    "# random_state=42 ensures reproducibility\n",
    "model = SGDClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the trained model on the testing set\n",
    "# This generates predictions for whether each employee in the test set will leave\n",
    "predictions = model.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5448484848484848"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the accuracy score of the model on the test set\n",
    "# This returns the proportion of correct predictions\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Model accuracy on test set: {accuracy:.4f}\")\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79701493, 0.77910448, 0.47761194, 0.75621891, 0.75323383,\n",
       "       0.45870647, 0.76119403, 0.66666667, 0.75422886, 0.76195219])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform 10-fold cross-validation on the training data\n",
    "# This splits the training data into 10 folds and trains/validates 10 times\n",
    "# This provides a more robust estimate of model performance\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions using 10-fold cross-validation\n",
    "# This returns predictions for each instance when it was in the validation fold\n",
    "# Useful for calculating metrics without overfitting\n",
    "y_train_pred = cross_val_predict(model, X_train, y_train, cv=10)\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "# Confusion matrix shows:\n",
    "# - True Negatives (top-left): Correctly predicted as staying\n",
    "# - False Positives (top-right): Incorrectly predicted as leaving\n",
    "# - False Negatives (bottom-left): Incorrectly predicted as staying\n",
    "# - True Positives (bottom-right): Correctly predicted as leaving\n",
    "\n",
    "conf_matrix = confusion_matrix(y_train, y_train_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"True Negatives (stayed, predicted stayed): {conf_matrix[0][0]}\")\n",
    "print(f\"False Positives (stayed, predicted left): {conf_matrix[0][1]}\")\n",
    "print(f\"False Negatives (left, predicted stayed): {conf_matrix[1][0]}\")\n",
    "print(f\"True Positives (left, predicted left): {conf_matrix[1][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the precision score\n",
    "# Precision = TP / (TP + FP)\n",
    "# Answers: \"Of all employees predicted to leave, what proportion actually left?\"\n",
    "# High precision means few false alarms\n",
    "precision = precision_score(y_train, y_train_pred)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the recall score (also known as sensitivity or true positive rate)\n",
    "# Recall = TP / (TP + FN)\n",
    "# Answers: \"Of all employees who actually left, what proportion did we correctly identify?\"\n",
    "# High recall means we catch most of the employees who will leave\n",
    "recall = recall_score(y_train, y_train_pred)\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the F1 score\n",
    "# F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "# F1 is the harmonic mean of precision and recall\n",
    "# Useful when you need a balance between precision and recall\n",
    "f1 = f1_score(y_train, y_train_pred)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate decision scores for each instance\n",
    "# decision_function returns the signed distance to the hyperplane\n",
    "# Higher scores indicate higher confidence that the instance belongs to the positive class\n",
    "y_scores = cross_val_predict(model, X_train, y_train, cv=10, method=\"decision_function\")\n",
    "y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision, recall, and thresholds for different decision thresholds\n",
    "# This allows us to analyze the precision-recall tradeoff\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)\n",
    "print(f\"Number of thresholds: {len(thresholds)}\")\n",
    "print(f\"Precision range: {precisions.min():.4f} to {precisions.max():.4f}\")\n",
    "print(f\"Recall range: {recalls.min():.4f} to {recalls.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the precision-recall curve to determine the optimal decision threshold\n",
    "# This helps visualize the tradeoff between precision and recall\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    \"\"\"\n",
    "    Plot precision and recall as functions of the decision threshold\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.xlabel(\"Threshold\", fontsize=12)\n",
    "    plt.ylabel(\"Score\", fontsize=12)\n",
    "    plt.title(\"Precision and Recall vs Decision Threshold\", fontsize=14)\n",
    "    plt.legend(loc=\"best\", fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.axhline(y=0.9, color='r', linestyle=':', alpha=0.5, label='90% line')\n",
    "    \n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.show()\n",
    "\n",
    "# Comments on threshold selection for employee retention context:\n",
    "# \n",
    "# For employee retention, we should prioritize RECALL over precision because:\n",
    "# 1. Missing an employee who will leave (False Negative) is costly - we lose the employee\n",
    "# 2. False alarms (False Positive) are less costly - we might offer retention incentives\n",
    "#    to someone who wasn't planning to leave, but this builds goodwill\n",
    "# 3. It's better to be proactive and intervene with more employees than to miss\n",
    "#    high-value employees who are planning to leave\n",
    "#\n",
    "# Recommended threshold: Lower than default (0) to achieve higher recall (e.g., 80-90%)\n",
    "# This ensures we identify most employees at risk of leaving, even if it means\n",
    "# some false positives. The cost of losing a good employee far outweighs the cost\n",
    "# of unnecessary retention efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the ROC (Receiver Operating Characteristic) curve\n",
    "# ROC curve plots True Positive Rate (Recall) vs False Positive Rate\n",
    "# A good classifier stays as close as possible to the top-left corner\n",
    "\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_train, y_scores)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label='ROC Curve')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')  # Diagonal line\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "plt.title('ROC Curve - Employee Retention Model', fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"True Positive Rate range: {tpr.min():.4f} to {tpr.max():.4f}\")\n",
    "print(f\"False Positive Rate range: {fpr.min():.4f} to {fpr.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Area Under the ROC Curve (AUC-ROC)\n",
    "# AUC-ROC measures the overall performance of the classifier\n",
    "# AUC = 1.0: Perfect classifier\n",
    "# AUC = 0.5: Random classifier (no better than coin flip)\n",
    "# AUC < 0.5: Worse than random (predictions are inverted)\n",
    "# Generally: AUC > 0.9 = Excellent, 0.8-0.9 = Good, 0.7-0.8 = Fair, < 0.7 = Poor\n",
    "\n",
    "roc_auc = roc_auc_score(y_train, y_scores)\n",
    "print(f\"Area Under the ROC Curve (AUC-ROC): {roc_auc:.4f}\")\n",
    "\n",
    "if roc_auc > 0.9:\n",
    "    print(\"Model Performance: Excellent\")\n",
    "elif roc_auc > 0.8:\n",
    "    print(\"Model Performance: Good\")\n",
    "elif roc_auc > 0.7:\n",
    "    print(\"Model Performance: Fair\")\n",
    "else:\n",
    "    print(\"Model Performance: Needs Improvement\")\n",
    "    \n",
    "roc_auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
