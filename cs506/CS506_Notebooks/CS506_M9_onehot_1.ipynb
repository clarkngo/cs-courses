{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building Vocabulary"
      ],
      "metadata": {
        "id": "hYvvpogm3sRC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TtiYcaJhoC1",
        "outputId": "120bc2d7-187a-46c0-d238-c6e251ebb1f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 I\n",
            "1 study\n",
            "2 at\n",
            "3 CityU\n",
            "4 Seattle\n"
          ]
        }
      ],
      "source": [
        "# book example on Listing 6.1 (one-hot encoding words)\n",
        "import numpy as np\n",
        "# 2 sentences as an input\n",
        "samples = ['I study at CityU', 'I study at CityU at Seattle']\n",
        "token_index = {} # builds an index of all tokens in the data using a dictionary for uniuqe words\n",
        "                 # key = word, value = running index from 1 -> N\n",
        "for sample in samples:\n",
        "    for word in sample.split(): # getting individual word from the each sentence\n",
        "        if word not in token_index:\n",
        "            #token_index[word] = len(token_index) + 1 # starting from 1\n",
        "            token_index[word] = len(token_index) # starting index at 0\n",
        "            print(token_index[word], word)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decide maximum number of words for a feature\n"
      ],
      "metadata": {
        "id": "2f7aCqaz_vL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# an arbitrary number of words you will consider for a feature\n",
        "# max_length = 6\n",
        "# Calculate the length of each sentence (number of words)\n",
        "sentence_lengths = [len(sentence.split()) for sentence in samples]\n",
        "# Find the maximum sentence length\n",
        "max_length = max(sentence_lengths)\n",
        "\n",
        "print(\"max_length: \", max_length)\n",
        "# we are creating a 3D matrix of samples x max_length x # of tokens\n",
        "results = np.zeros(shape = (len(samples), max_length, max(token_index.values()) + 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbB0Rmj1_7ha",
        "outputId": "38bbe16b-80bd-4c6f-d3e0-a536a577bf31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_length:  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot encoding"
      ],
      "metadata": {
        "id": "vCO4JQyl30tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through each sample with its index (i)\n",
        "for i, sample in enumerate(samples):\n",
        "    # Iterate through each word in the sample with its index (j), up to max_length\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        # Get the index of the current word from the token_index dictionary\n",
        "        index = token_index.get(word)\n",
        "        # Perform one-hot encoding: set the corresponding element in the results array to 1.0\n",
        "        results[i, j, index] = 1.\n",
        "\n",
        "# The results array now contains the one-hot encoded representation of the samples\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPGAdFQg4cIJ",
        "outputId": "7dfe4d51-a8a0-4cc9-d915-3d7f92e3457c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[1., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 1.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Vocabulary and one-hot-encoding using keras"
      ],
      "metadata": {
        "id": "rTCeBA2BAxoQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x4534ojhoC3",
        "outputId": "aa102ae7-9793-47b8-9412-b36eda5ad374",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 unique tokens.\n",
            "word_index:  {'at': 1, 'i': 2, 'study': 3, 'cityu': 4, 'seattle': 5}\n",
            "\u001b[1m\u001b[94m{ Sequences: }\u001b[0m  [[2, 3, 1, 4], [2, 3, 1, 4, 1, 5]]\n",
            "\n",
            "\u001b[1m\u001b[94m{ one hot results: }\u001b[0m\n",
            " [[0. 1. 1. 1. 1. 0.]\n",
            " [0. 1. 1. 1. 1. 1.]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "samples = ['I study at CityU', 'I study at CityU at Seattle']\n",
        "\n",
        "sentence_lengths = [len(sentence.split()) for sentence in samples]\n",
        "# Find the maximum sentence length\n",
        "max_length = max(sentence_lengths)\n",
        "\n",
        "# Let's create a tokenizer, configured to only take into account the top-1000 most common words\n",
        "tokenizer = Tokenizer(num_words = max_length)\n",
        "# Updates internal vocabulary based on a list of texts. This method creates the\n",
        "# vocabulary based on word frequency.\n",
        "# The output of the vocabulary is word_index[\"I\"] = 1, word_index[\"study\"] = 2,\n",
        "# word(key):index(value)\n",
        "tokenizer.fit_on_texts(samples)\n",
        "# Transforms those strings into a sequence of interger indices.\n",
        "# Basically, it takes each word in the text and replaces it with\n",
        "# its corresponding integer value from the word_index dictionary\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "# Get the one-hot binary representation of given sentences\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode = 'binary') # mode='count'\n",
        "# one_hot_results = tokenizer.texts_to_sequences(samples) # another helper function to produce the encoded sequence\n",
        "# Obtain the word index that was computed\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' %len(word_index))\n",
        "print(\"word_index: \", tokenizer.word_index)\n",
        "print(\"\\033[1m\\033[94m{ Sequences: }\\033[0m \", sequences)\n",
        "print(\"\\n\\033[1m\\033[94m{ one hot results: }\\033[0m\\n\", one_hot_results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHZu08YFhoC3",
        "outputId": "7680cbe7-e352-43b8-c26e-0a8ccd305eb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., 1., 1., 0.],\n",
              "       [0., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "one_hot_results\n"
      ]
    },
    {
      "source": [
        "word embedding examples using an embedding layer in Keras\n",
        "learning an embedding layer\n",
        "source: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "wV7S0Ab8hoC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
        "from keras.models import Sequential\n",
        "\n",
        "# define documents\n",
        "docs = ['Well done!',           # + = 1\n",
        "\t\t'Good work',\n",
        "\t\t'Great effort',\n",
        "\t\t'nice work',\n",
        "\t\t'Excellent!',\n",
        "\t\t'Fine work!',\n",
        "        'Bravo!',\n",
        "        'Tremendous idea',\n",
        "        'Awesome!',\n",
        "        'Perfect work',\n",
        "        'Weak',                 # - = 0\n",
        "\t\t'Poor effort!',\n",
        "\t\t'not good',\n",
        "\t\t'poor work',\n",
        "\t\t'Could have done better.',\n",
        "        'Sucks',\n",
        "        'Inferior to your previous work',\n",
        "        'Substandard',\n",
        "        'Faulty thoughts',\n",
        "        'Terrible work to be presented'\n",
        "        ]\n",
        "# define class labels\n",
        "labels = array([1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0])\n",
        "# integer encode the documents (hash\n",
        "vocab_size = 1000 # hyper parameter#1 affects hash collision (make sure you have enough entries to avoid any hash collision)\n",
        "# one_hot converts an input sentence into a vector\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print(encoded_docs)\n",
        "# pad documents to a mavx length of 4 words\n",
        "max_length = 8 # vector space to accommodate the input text sequence, [1, max_length].\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)\n",
        "# define the model\n",
        "model = Sequential()\n",
        "# This Embedding has a vocabulary of 50 and an input length of 4. We will choose a small embedding space of 8 dimensions.\n",
        "num_layer = 8 # hyper parameter#2 {8, 16, 24, 128 ...}\n",
        "model.add(Embedding(vocab_size, num_layer, input_length=max_length))\n",
        "# Importantly, the output from the Embedding layer will be 4 vectors of 8 dimensions each, one for each word. We flatten this to a one 32-element vector to pass on to the Dense output layer.\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))\n",
        "\n",
        "# The model architecture, and training configuration (including the optimizer, losses, and metrics) are stored in saved_model.pb.\n",
        "# The weights are saved in the variables/ directory.\n",
        "model.save('my_model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "PMILt32T76F_",
        "outputId": "ac5c9c1f-f02d-40ed-c24e-ba3b0de6afeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[432, 957], [895, 588], [117, 971], [559, 588], [282], [198, 588], [126], [640, 264], [566], [43, 588], [188], [38, 971], [303, 895], [38, 588], [315, 936, 957, 630], [806], [257, 435, 267, 510, 588], [659], [985, 445], [65, 588, 435, 301, 954]]\n",
            "[[432 957   0   0   0   0   0   0]\n",
            " [895 588   0   0   0   0   0   0]\n",
            " [117 971   0   0   0   0   0   0]\n",
            " [559 588   0   0   0   0   0   0]\n",
            " [282   0   0   0   0   0   0   0]\n",
            " [198 588   0   0   0   0   0   0]\n",
            " [126   0   0   0   0   0   0   0]\n",
            " [640 264   0   0   0   0   0   0]\n",
            " [566   0   0   0   0   0   0   0]\n",
            " [ 43 588   0   0   0   0   0   0]\n",
            " [188   0   0   0   0   0   0   0]\n",
            " [ 38 971   0   0   0   0   0   0]\n",
            " [303 895   0   0   0   0   0   0]\n",
            " [ 38 588   0   0   0   0   0   0]\n",
            " [315 936 957 630   0   0   0   0]\n",
            " [806   0   0   0   0   0   0   0]\n",
            " [257 435 267 510 588   0   0   0]\n",
            " [659   0   0   0   0   0   0   0]\n",
            " [985 445   0   0   0   0   0   0]\n",
            " [ 65 588 435 301 954   0   0   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Accuracy: 100.000000\n"
          ]
        }
      ]
    },
    {
      "source": [
        "How to use pre-trained network (GloVe) in Keras"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmorcq7WhoC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# The URL for the GloVe 6B dataset (specifically the 100d version)\n",
        "# This is a common source, but URLs can change. If this link fails,\n",
        "# you might need to find an alternative source or download it manually.\n",
        "glove_url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "zip_file_name = \"glove.6B.zip\"\n",
        "extracted_file_name = \"glove.6B.100d.txt\"\n",
        "\n",
        "print(f\"Attempting to download GloVe embeddings from: {glove_url}\")\n",
        "\n",
        "# Download the zip file\n",
        "try:\n",
        "    response = requests.get(glove_url, stream=True)\n",
        "    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    with open(zip_file_name, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "    print(f\"Downloaded {zip_file_name}\")\n",
        "\n",
        "    # Extract the specific file\n",
        "    with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "        if extracted_file_name in zip_ref.namelist():\n",
        "            zip_ref.extract(extracted_file_name)\n",
        "            print(f\"Extracted {extracted_file_name}\")\n",
        "        else:\n",
        "            print(f\"Error: {extracted_file_name} not found in the zip archive.\")\n",
        "            print(\"Available files in the archive:\", zip_ref.namelist())\n",
        "\n",
        "    # Clean up the zip file (optional)\n",
        "    # os.remove(zip_file_name)\n",
        "    # print(f\"Removed temporary zip file {zip_file_name}\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading the file: {e}\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: Downloaded file {zip_file_name} is not a valid zip file.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhBFMdEc9fVF",
        "outputId": "fe997ed7-0903-4f67-91ee-06fb86d4e8fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to download GloVe embeddings from: https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Downloaded glove.6B.zip\n",
            "Extracted glove.6B.100d.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "# Corrected imports from tensorflow.keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Embedding\n",
        "# define documents\n",
        "docs = ['Well done!',\n",
        "\t\t'Good work',\n",
        "\t\t'Great effort',\n",
        "\t\t'nice work',\n",
        "\t\t'Excellent!',\n",
        "\t\t'Weak',\n",
        "\t\t'Poor effort!',\n",
        "\t\t'not good',\n",
        "\t\t'poor work',\n",
        "\t\t'Could have done better.']\n",
        "# define class labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
        "# prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(docs)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "# integer encode the documents\n",
        "encoded_docs = t.texts_to_sequences(docs)\n",
        "print(\"\\033[1m\\033[94m{ Encoded Documents: }\\033[0m \", encoded_docs)\n",
        "# pad documents to a max length of 4 words\n",
        "#max_length = 4\n",
        "\n",
        "# Calculate the length of each sentence (number of words)\n",
        "sentence_lengths = [len(sentence.split()) for sentence in docs]\n",
        "\n",
        "# Find the maximum sentence length\n",
        "max_length = max(sentence_lengths)\n",
        "\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(\"\\033[1m\\033[94m{ Padded Documents: }\\033[0m \", padded_docs)\n",
        "\n",
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "# Ensure the path to the GloVe file is correct for your environment\n",
        "# If the file is not in the same directory, update the path accordingly\n",
        "# Example: f = open('/path/to/your/glove.6B.100d.txt', encoding='utf8')\n",
        "try:\n",
        "    f = open('glove.6B.100d.txt', encoding='utf8')\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: glove.6B.100d.txt not found.\")\n",
        "    print(\"Please download the GloVe embeddings from https://nlp.stanford.edu/projects/glove/\")\n",
        "    print(\"and place the glove.6B.100d.txt file in the same directory as the notebook,\")\n",
        "    print(\"or update the file path in the code.\")\n",
        "    # Exit or handle the error appropriately if the file is crucial\n",
        "    # sys.exit(\"GloVe file not found.\") # Uncomment this line to stop execution if the file is missing\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, i in t.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector\n",
        "# define model\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "2AiGhaUj89Uz",
        "outputId": "ec168848-bcfc-4df4-95e4-423eb05c3a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[94m{ Encoded Documents: }\u001b[0m  [[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n",
            "\u001b[1m\u001b[94m{ Padded Documents: }\u001b[0m  [[ 6  2  0  0]\n",
            " [ 3  1  0  0]\n",
            " [ 7  4  0  0]\n",
            " [ 8  1  0  0]\n",
            " [ 9  0  0  0]\n",
            " [10  0  0  0]\n",
            " [ 5  4  0  0]\n",
            " [11  3  0  0]\n",
            " [ 5  1  0  0]\n",
            " [12 13  2 14]]\n",
            "Loaded 400000 word vectors.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │         \u001b[38;5;34m1,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,500\u001b[0m (5.86 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,500</span> (5.86 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,500\u001b[0m (5.86 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,500</span> (5.86 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Accuracy: 100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrcsDrfyhoC5"
      },
      "outputs": [],
      "source": []
    }
  ]
}