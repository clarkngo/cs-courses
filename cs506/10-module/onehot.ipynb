{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 I\n2 study\n3 at\n4 CityU\n5 Seattle\n"
     ]
    }
   ],
   "source": [
    "# book example on Listing 6.1 (one-hot encoding words)\n",
    "import numpy as np\n",
    "# 2 sentences as an input\n",
    "samples = ['I study at CityU', 'I study at CityU at Seattle']\n",
    "token_index = {} # builds an index of all tokens in the data using a dictionary for uniuqe words\n",
    "                 # key = word, value = running index from 1 -> N\n",
    "for sample in samples:\n",
    "    for word in sample.split(): # getting individual word from the each sentence\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1 # starting from 1\n",
    "            print(token_index[word], word)\n",
    "# an arbitrary number of words you will consider for a feature\n",
    "max_length = 6 \n",
    "# we are creating a 3D matrix of samples x max_length x # of tokens\n",
    "results = np.zeros(shape = (len(samples), max_length, max(token_index.values()) + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1. \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 5 unique tokens.\nSequences:  [[2, 3, 1, 4], [2, 3, 1, 4, 1, 5]] \n\nword_index:  {'at': 1, 'i': 2, 'study': 3, 'cityu': 4, 'seattle': 5}\none hot results:  [[0. 1. 1. 1. 1. 0.]\n [0. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "samples = ['I study at CityU', 'I study at CityU at Seattle']\n",
    "\n",
    "# Let's create a tokenizer, configured to only take into account the top-1000 most common words\n",
    "tokenizer = Tokenizer(num_words = 6)\n",
    "# Updates internal vocabulary based on a list of texts. This method creates the\n",
    "# vocabulary based on word frequency.\n",
    "# The output of the vocabulary is word_index[\"I\"] = 1, word_index[\"study\"] = 2,\n",
    "# word(key):index(value) \n",
    "tokenizer.fit_on_texts(samples)\n",
    "# Transforms those strings into a sequence of interger indices.\n",
    "# Basically, it takes each word in the text and replaces it with \n",
    "# its corresponding integer value from the word_index dictionary\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "# Get the one-hot binary representation of given sentences\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode = 'binary') # mode='count'\n",
    "# one_hot_results = tokenizer.texts_to_sequences(samples) # another helper function to produce the encoded sequence\n",
    "# Obtain the word index that was computed\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' %len(word_index))\n",
    "print(\"Sequences: \", sequences, \"\\n\")\n",
    "print(\"word_index: \", tokenizer.word_index)\n",
    "print(\"one hot results: \", one_hot_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "one_hot_results\n"
   ]
  },
  {
   "source": [
    "word embedding examples using an embedding layer in Keras\n",
    "learning an embedding layer\n",
    "source: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[515, 675], [611, 816], [432, 214], [537, 816], [641], [80, 816], [394], [916, 462], [208], [246, 816], [333], [356, 214], [781, 611], [356, 816], [287, 800, 675, 825], [770], [647, 268, 825, 222, 816], [521], [118, 617], [991, 816, 268, 948, 997]]\n",
      "[[515 675   0   0   0   0   0   0]\n",
      " [611 816   0   0   0   0   0   0]\n",
      " [432 214   0   0   0   0   0   0]\n",
      " [537 816   0   0   0   0   0   0]\n",
      " [641   0   0   0   0   0   0   0]\n",
      " [ 80 816   0   0   0   0   0   0]\n",
      " [394   0   0   0   0   0   0   0]\n",
      " [916 462   0   0   0   0   0   0]\n",
      " [208   0   0   0   0   0   0   0]\n",
      " [246 816   0   0   0   0   0   0]\n",
      " [333   0   0   0   0   0   0   0]\n",
      " [356 214   0   0   0   0   0   0]\n",
      " [781 611   0   0   0   0   0   0]\n",
      " [356 816   0   0   0   0   0   0]\n",
      " [287 800 675 825   0   0   0   0]\n",
      " [770   0   0   0   0   0   0   0]\n",
      " [647 268 825 222 816   0   0   0]\n",
      " [521   0   0   0   0   0   0   0]\n",
      " [118 617   0   0   0   0   0   0]\n",
      " [991 816 268 948 997   0   0   0]]\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_28 (Embedding)     (None, 8, 8)              8000      \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 8,065\n",
      "Trainable params: 8,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000001C2900191F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Accuracy: 94.999999\n",
      "WARNING:tensorflow:From C:\\Users\\usd20311\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\usd20311\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: my_model\\assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "# define documents\n",
    "docs = ['Well done!',           # + = 1\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Fine work!',\n",
    "        'Bravo!',\n",
    "        'Tremendous idea',\n",
    "        'Awesome!',\n",
    "        'Perfect work',\n",
    "        'Weak',                 # - = 0\n",
    "\t\t'Poor effort!',         \n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.',\n",
    "        'Sucks',\n",
    "        'Inferior to your previous work',\n",
    "        'Substandard',\n",
    "        'Faulty thoughts',\n",
    "        'Terrible work to be presented'        \n",
    "        ]\n",
    "# define class labels\n",
    "#labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "labels = array([1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0])\n",
    "# integer encode the documents (hash \n",
    "vocab_size = 1000 # hyper parameter#1 affects hash collision (make sure you have enough entries to avoid any hash collision)\n",
    "# one_hot converts an input sentence into a vector\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)\n",
    "# pad documents to a mavx length of 4 words\n",
    "max_length = 8 # vector space to accommodate the input text sequence, [1, max_length].\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# define the model\n",
    "model = Sequential()\n",
    "# This Embedding has a vocabulary of 50 and an input length of 4. We will choose a small embedding space of 8 dimensions.\n",
    "num_layer = 8 # hyper parameter#2 {8, 16, 24, 128 ...}\n",
    "model.add(Embedding(vocab_size, num_layer, input_length=max_length))\n",
    "# Importantly, the output from the Embedding layer will be 4 vectors of 8 dimensions each, one for each word. We flatten this to a one 32-element vector to pass on to the Dense output layer.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "# The model architecture, and training configuration (including the optimizer, losses, and metrics) are stored in saved_model.pb. \n",
    "# The weights are saved in the variables/ directory.\n",
    "model.save('my_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "How to use pre-trained network (GloVe) in Keras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B\\\\glove.6B.100d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector\n",
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}